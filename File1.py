# -*- coding: utf-8 -*-
"""Accredian.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SNfFsqyF06bFdu7TzpxlKUimn8LgoLBz
"""

# from google.colab import drive
# drive.mount('/content/drive')

# pip install pandas numpy matplotlib seaborn
import pandas as pd

import pandas as pd

# Load the dataset
file_path = "Fraud.csv"

df = pd.read_csv(file_path)

# Display the first few rows
print("First few rows of the dataset:")
print(df.head())

# Check the shape of the dataframe
print("Shape of the dataframe ", df.shape)

import numpy as np

# Function to remove outliers using Z-score method
def remove_outliers_zscore(df, columns, threshold=3):
    z_scores = np.abs((df[columns] - df[columns].mean()) / df[columns].std())
    filtered_entries = (z_scores < threshold).all(axis=1)
    return df[filtered_entries]

# List of numerical columns to check for outliers
numerical_columns = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']

# Remove outliers from the dataset
df_no_outliers = remove_outliers_zscore(df, numerical_columns)


# Assuming 'df' is your DataFrame containing the dataset
df_no_outliers = df_no_outliers.drop(columns=['newbalanceOrig', 'newbalanceDest'])
print(df.shape)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import OneHotEncoder

# One-hot encode the 'type' column
df_encoded = pd.get_dummies(df_no_outliers, columns=['type'])

# Split the data into features (X) and target variable (y)
X = df_encoded.drop(columns=['isFraud', 'nameOrig', 'nameDest'])
y = df_encoded['isFraud']

print("df Encoded")



from xgboost import XGBClassifier
from sklearn.ensemble import GradientBoostingClassifier
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# the target variable 'isFraud' is also defined

# Split the data into features and target variable
X = df_no_outliers.drop(columns=['isFraud'])
y = df_no_outliers['isFraud']

# Convert categorical variable 'type' to numeric using one-hot encoding
X = pd.get_dummies(X, columns=['type'])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("df splitted in train test")

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'class_weight': ['balanced']
}

# Create the GridSearchCV object
grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), 
                           param_grid=param_grid, 
                           cv=3, 
                           n_jobs=-1, 
                           scoring='f1')

print("Model fitting")

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
best_params = grid_search.best_params_
print("Best parameters found: ", best_params)

# Train the Random Forest model with the best parameters
best_rf = RandomForestClassifier(**best_params, random_state=42)
best_rf.fit(X_train, y_train)

# Make predictions on the testing set
y_pred_best_rf = best_rf.predict(X_test)

# Evaluate the model
accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)
precision_best_rf = precision_score(y_test, y_pred_best_rf, zero_division=1)
recall_best_rf = recall_score(y_test, y_pred_best_rf, zero_division=1)
f1_best_rf = f1_score(y_test, y_pred_best_rf, zero_division=1)

print("Best Random Forest Model:")
print(f"Accuracy: {accuracy_best_rf}")
print(f"Precision: {precision_best_rf}")
print(f"Recall: {recall_best_rf}")
print(f"F1-score: {f1_best_rf}")

# Identify key factors predicting fraudulent transactions
importances = best_rf.feature_importances_
features = X_train.columns

# Create a DataFrame for feature importances
feature_importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Display the feature importances
print(feature_importance_df)

 